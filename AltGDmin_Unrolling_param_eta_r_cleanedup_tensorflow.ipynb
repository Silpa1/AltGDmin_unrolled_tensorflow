{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcTpXa_CE3ex",
        "outputId": "07c982bb-7fde-4e3d-d957-d42fcdf14fb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
        "\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import scipy.sparse.linalg as linalg\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "# Filter out specific TensorFlow warnings\n",
        "warnings.filterwarnings('ignore', message=\"Gradients do not exist for variables\")\n",
        "# Set TensorFlow logging to ERROR only\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow logging (1: INFO, 2: WARNING, 3: ERROR)\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "# Ensure TensorFlow is using GPU if available\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "  except RuntimeError as e:\n",
        "    print(e)\n",
        "\n",
        "# Use float64 for all operations\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "# ================Parameters======================\n",
        "r = 5        # underlying rank\n",
        "n = 200      # size (num. of rows)\n",
        "q = 200      # size (num. of columns)\n",
        "m = 60       # size (number of measurements)\n",
        "step_initial = 0.5    # initial value of step size (eta in the paper)\n",
        "maxIt = 100    # num. of layers you want to train\n",
        "thr_initial = 0.7\n",
        "\n",
        "# =============Generate Data y_k = A_k U b_k=============\n",
        "def generate_problem(r, n, q):\n",
        "    U0_t = tf.random.normal((n, r), dtype=tf.float64) / math.sqrt(n)\n",
        "    U0_t, _ = tf.linalg.qr(U0_t)\n",
        "    B0_t = tf.random.normal((r, q), dtype=tf.float64) / math.sqrt(q)\n",
        "    A = tf.random.normal((m, n, q), dtype=tf.float64) / math.sqrt(m)\n",
        "    noise = tf.random.normal((n, q), dtype=tf.float64) / math.sqrt(n * q)\n",
        "    Y0_t_array = tf.TensorArray(dtype=tf.float64, size=q)\n",
        "    for k in range(q):\n",
        "        yk= tf.matmul(A[:, :, k], tf.matmul(U0_t, B0_t[:, k][:, tf.newaxis]))[:, 0]\n",
        "        Y0_t_array = Y0_t_array.write(k, yk)\n",
        "    # Convert the TensorArray to a Tensor\n",
        "    Y0_t = Y0_t_array.stack()\n",
        "    Y0_t = tf.transpose(Y0_t)  # Make sure the shape matches the expected (m, q)\n",
        "\n",
        "    return U0_t, B0_t, Y0_t, A\n",
        "\n",
        "\n",
        "class MatNet(tf.keras.Model):\n",
        "    def __init__(self, step_initial, thr_initial):\n",
        "        super(MatNet, self).__init__()\n",
        "        #self.maxIt = maxIt\n",
        "        self.step_initial = step_initial\n",
        "        self.thr_initial = thr_initial\n",
        "\n",
        "        # Define the parameters to be learned\n",
        "        self.step = [self.add_weight(name=f'step_{t}',\n",
        "                                     shape=(),\n",
        "                                     initializer=tf.constant_initializer(step_initial),\n",
        "                                     dtype=tf.float64,\n",
        "                                     trainable=True) for t in range(maxIt)]\n",
        "\n",
        "        self.thr = [self.add_weight(name='thr',\n",
        "                                    shape=(),\n",
        "                                    initializer=tf.constant_initializer(thr_initial),\n",
        "                                    dtype=tf.float64,\n",
        "                                    trainable=True)]\n",
        "    def lowrank(self, X0, threshold):\n",
        "        X0 = tf.identity(X0)  # Clone the tensor\n",
        "        St, Ut, Vt = tf.linalg.svd(X0, compute_uv=True)\n",
        "        #print(threshold)\n",
        "        thres = threshold * St[0]\n",
        "        #print(f\"thres: {thres}\")\n",
        "        St = tf.maximum(St - thres, 0)  # Equivalent to relu(St - thres)\n",
        "        St = tf.cast(St, tf.float64)  # Ensure St is of float type\n",
        "        r = tf.math.count_nonzero(St)\n",
        "        #print(f\"r: {r}\")\n",
        "        #print(f\"St: {St}\")\n",
        "        S_diag = tf.linalg.diag(St)\n",
        "        Xinit = Ut @ S_diag @ tf.transpose(Vt)\n",
        "        # Since TensorFlow does not have svd_lowrank, we recompute SVD for Xinit as a demonstration.\n",
        "        # This might not be exactly what you want, so adjust according to your specific needs.\n",
        "        St, Ut, Vt = tf.linalg.svd(Xinit, compute_uv=True)\n",
        "        return tf.cast(Ut[:,0:r], tf.float64)  # Cast Ut to double\n",
        "    def call(self, Y0_t, U0_t, B0_t, A, num_l):\n",
        "        # Initialization\n",
        "        n, r0 = U0_t.shape\n",
        "        #print(f\"r0: {r0}\")\n",
        "        r0, q = B0_t.shape\n",
        "        # Initialize X0 as a list of tensors to be concatenated later\n",
        "        X0_list = []\n",
        "        for k in range(q):\n",
        "            X0_k = tf.matmul(tf.transpose(A[:, :, k]), Y0_t[:, k][:, tf.newaxis])\n",
        "            X0_list.append(X0_k[:, 0])  # Remove the extra dimension\n",
        "        X0 = tf.stack(X0_list, axis=1)\n",
        "        U_t = self.lowrank(X0, self.thr[0])\n",
        "        n, r = U_t.shape\n",
        "        #print(r)\n",
        "        B_t_list = []\n",
        "        for k in range(q):\n",
        "            B_t_k = tf.matmul(tf.linalg.pinv(tf.matmul(A[:, :, k], U_t)), Y0_t[:, k][:, tf.newaxis])  # Column-wise Least Squares\n",
        "            B_t_list.append(B_t_k[:, 0])\n",
        "        B_t = tf.stack(B_t_list, axis=1)\n",
        "        # Main Loop\n",
        "        for t in range(1, num_l):\n",
        "            E_t = tf.zeros((n, r), dtype=tf.float64)\n",
        "            for k in range(q):\n",
        "                AkU = tf.matmul(A[:, :, k], U_t)\n",
        "                Yk_minus_AkU_Bk = Y0_t[:, k] - tf.matmul(AkU, B_t[:, k][:, tf.newaxis])[:, 0]\n",
        "                E_t += tf.matmul(A[:, :, k], Yk_minus_AkU_Bk[:, tf.newaxis], transpose_a=True) * B_t[:, k]\n",
        "\n",
        "            #print(f\"Gradient: {E_t}\")\n",
        "            Unew = tf.linalg.qr(U_t + self.step[t] * E_t)[0]  # Projected Gradient Descent\n",
        "            Bnew_list = []\n",
        "            for k in range(q):\n",
        "                Bnew_k = tf.matmul(tf.linalg.pinv(tf.matmul(A[:, :, k], Unew)), Y0_t[:, k][:, tf.newaxis])  # Column-wise Least Squares\n",
        "                Bnew_list.append(Bnew_k[:, 0])\n",
        "            Bnew = tf.stack(Bnew_list, axis=1)\n",
        "            U_t, B_t = Unew, Bnew\n",
        "\n",
        "        Y_t_list = []\n",
        "        for k in range(q):\n",
        "            Y_t_k = tf.matmul(A[:, :, k], tf.matmul(U_t, B_t[:, k][:, tf.newaxis]))[:, 0]\n",
        "            Y_t_list.append(Y_t_k)\n",
        "        Y_t = tf.stack(Y_t_list, axis=1)\n",
        "        loss = tf.norm(Y_t - Y0_t)\n",
        "        #print(loss)\n",
        "        return loss, U_t, B_t\n",
        "    def enable_single_layer(self, layer_index):\n",
        "    # Assuming 'model' is a tf.keras.Sequential model or a model that supports indexing.\n",
        "      for i, layer in enumerate(self.layers):\n",
        "        layer.trainable = (i == layer_index)\n",
        "    def enable_layers(self, num_layers):\n",
        "    # Enable first 'num_layers' and disable the rest.\n",
        "      for i, layer in enumerate(self.layers):\n",
        "        layer.trainable = i < num_layers\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "Nepoches_pre = 4\n",
        "Nepoches_full = 4\n",
        "lr_fac = 1.0  # basic learning rate\n",
        "maxIt = 10 # define the maximum iterations based on your model's depth\n",
        "\n",
        "net = MatNet(step_initial, thr_initial)\n",
        "start = time.time()\n",
        "\n",
        "for stage in range(1, maxIt):\n",
        "    print(f'Layer {stage}, Pre-training ======================')\n",
        "    if stage > 6:\n",
        "        Nepoches_full = 2\n",
        "\n",
        "    net.enable_single_layer( stage)\n",
        "    # Define a separate optimizer for this stage\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_fac * 0.01 if stage == 0 else lr_fac * 0.1)\n",
        "    for epoch in range(Nepoches_pre):\n",
        "        U0_t, B0_t, Y0_t, A = generate_problem(r, n, q)\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss, U_t, B_t = net(Y0_t, U0_t, B0_t, A, stage + 1)  # Adjust based on your model's method signature\n",
        "            gradients = tape.gradient(loss, net.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, net.trainable_variables))\n",
        "\n",
        "        if epoch % 2 == 0:\n",
        "            print(f\"epoch: {epoch} \\t loss: {loss.numpy()}\")\n",
        "\n",
        "    print(f'Layer {stage}, Full-training =====================')\n",
        "    net.enable_layers(stage + 1)\n",
        "    for epoch in range(Nepoches_full):\n",
        "        U0_t, B0_t, Y0_t, A = generate_problem(r, n, q)\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss, U_t, B_t = net(Y0_t, U0_t, B0_t, A, stage + 1)  # Adjust based on your model's method signature\n",
        "            gradients = tape.gradient(loss, net.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, net.trainable_variables))\n",
        "\n",
        "        if epoch % 2 == 0:\n",
        "            print(f\"epoch: {epoch} \\t loss: {loss.numpy()}\")\n",
        "\n",
        "end = time.time()\n",
        "print(f\"Training end. Time: {end - start}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbYPkH77HtLK",
        "outputId": "2392fff3-753f-4a8b-a5ff-861bba3e1aa1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1, Pre-training ======================\n",
            "epoch: 0 \t loss: 0.462218318014601\n",
            "epoch: 2 \t loss: 0.38696277976393384\n",
            "Layer 1, Full-training =====================\n",
            "epoch: 0 \t loss: 0.27695845083330706\n",
            "epoch: 2 \t loss: 0.2793890824935758\n",
            "Layer 2, Pre-training ======================\n",
            "epoch: 0 \t loss: 0.13551270823753866\n",
            "epoch: 2 \t loss: 0.12119772680084283\n",
            "Layer 2, Full-training =====================\n",
            "epoch: 0 \t loss: 0.12256420289648025\n",
            "epoch: 2 \t loss: 0.11994938553310999\n",
            "Layer 3, Pre-training ======================\n",
            "epoch: 0 \t loss: 0.06425096751506156\n",
            "epoch: 2 \t loss: 0.05858813710029684\n",
            "Layer 3, Full-training =====================\n",
            "epoch: 0 \t loss: 0.048606123434752596\n",
            "epoch: 2 \t loss: 0.05587164654596481\n",
            "Layer 4, Pre-training ======================\n",
            "epoch: 0 \t loss: 0.03552836213153395\n",
            "epoch: 2 \t loss: 0.027162113972454525\n",
            "Layer 4, Full-training =====================\n",
            "epoch: 0 \t loss: 0.026853590439603024\n",
            "epoch: 2 \t loss: 0.0237681482546749\n",
            "Layer 5, Pre-training ======================\n",
            "epoch: 0 \t loss: 0.017257008488158147\n",
            "epoch: 2 \t loss: 0.014179774989257017\n",
            "Layer 5, Full-training =====================\n",
            "epoch: 0 \t loss: 0.00997913420263392\n",
            "epoch: 2 \t loss: 0.014311244510375572\n",
            "Layer 6, Pre-training ======================\n",
            "epoch: 0 \t loss: 0.010234404480537284\n",
            "epoch: 2 \t loss: 0.013233178168354719\n",
            "Layer 6, Full-training =====================\n",
            "epoch: 0 \t loss: 0.006362107270047809\n",
            "epoch: 2 \t loss: 0.010025825864593012\n",
            "Layer 7, Pre-training ======================\n",
            "epoch: 0 \t loss: 0.009287095059724821\n",
            "epoch: 2 \t loss: 0.003780261009188623\n",
            "Layer 7, Full-training =====================\n",
            "epoch: 0 \t loss: 0.023020576531708737\n",
            "Layer 8, Pre-training ======================\n",
            "epoch: 0 \t loss: 0.004334361579738142\n",
            "epoch: 2 \t loss: 0.011091506780779201\n",
            "Layer 8, Full-training =====================\n",
            "epoch: 0 \t loss: 0.002801116121457429\n",
            "Layer 9, Pre-training ======================\n",
            "epoch: 0 \t loss: 0.002060820683972943\n",
            "epoch: 2 \t loss: 0.0016537307210149788\n",
            "Layer 9, Full-training =====================\n",
            "epoch: 0 \t loss: 0.002246718844029439\n",
            "Training end. Time: 1327.4414083957672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.trainable_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05RF3JcdJqsQ",
        "outputId": "c79ccad5-37e1-45f5-9f7d-966567b15eac"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'step_0:0' shape=() dtype=float64, numpy=0.5>,\n",
              " <tf.Variable 'step_1:0' shape=() dtype=float64, numpy=3.3549546879366843>,\n",
              " <tf.Variable 'step_2:0' shape=() dtype=float64, numpy=0.5292875623123038>,\n",
              " <tf.Variable 'step_3:0' shape=() dtype=float64, numpy=1.7927214018335142>,\n",
              " <tf.Variable 'step_4:0' shape=() dtype=float64, numpy=1.1939650406274556>,\n",
              " <tf.Variable 'step_5:0' shape=() dtype=float64, numpy=0.603998130128475>,\n",
              " <tf.Variable 'step_6:0' shape=() dtype=float64, numpy=0.6594405072753532>,\n",
              " <tf.Variable 'step_7:0' shape=() dtype=float64, numpy=0.830699251849397>,\n",
              " <tf.Variable 'step_8:0' shape=() dtype=float64, numpy=0.5604643375089539>,\n",
              " <tf.Variable 'step_9:0' shape=() dtype=float64, numpy=0.5262913880991335>,\n",
              " <tf.Variable 'thr:0' shape=() dtype=float64, numpy=0.7000000000162544>]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}